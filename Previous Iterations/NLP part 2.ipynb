{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674c3f38-dab5-4fc3-b82d-ffdf3c48b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2de62a-b292-4f55-a53e-07e063bc5187",
   "metadata": {},
   "source": [
    "## Reading Dataset and Cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d95a6f-4271-4e4f-88a3-4afa5162fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('jokes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc9870c-3c5a-4740-9fde-c9535d0e8434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Did you hear about the Native American man tha...</td>\n",
       "      <td>He nearly drown in his own tea pee.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What's the best anti diarrheal prescription?</td>\n",
       "      <td>Mycheexarphlexin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What do you call a person who is outside a doo...</td>\n",
       "      <td>Matt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Which Star Trek character is a member of the m...</td>\n",
       "      <td>Jean-Luc Pickacard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What's the difference between a bullet and a h...</td>\n",
       "      <td>A bullet doesn't miss Harambe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           Question  \\\n",
       "0   1  Did you hear about the Native American man tha...   \n",
       "1   2       What's the best anti diarrheal prescription?   \n",
       "2   3  What do you call a person who is outside a doo...   \n",
       "3   4  Which Star Trek character is a member of the m...   \n",
       "4   5  What's the difference between a bullet and a h...   \n",
       "\n",
       "                                Answer  \n",
       "0  He nearly drown in his own tea pee.  \n",
       "1                     Mycheexarphlexin  \n",
       "2                                 Matt  \n",
       "3                   Jean-Luc Pickacard  \n",
       "4        A bullet doesn't miss Harambe  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad53ff7c-2afa-4348-a39c-319ba8e7fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ID'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbdc6cb-7079-40c4-9cdc-d2e59b52d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kunwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kunwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kunwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kunwe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_nltk=__import__('nltk')\n",
    "foo_nltk.download('stopwords')\n",
    "foo_nltk.download('punkt')\n",
    "foo_nltk.download('wordnet')\n",
    "foo_nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f2b1b1-0608-467b-9b45-1213de634b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower() #convert all the chracters into small letters\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}'+=|.!?,]\", \"\", text)\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    return clean(text,no_emoji=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efe2f19-e785-42c6-9427-16f05e3f2a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>cleaned_Q</th>\n",
       "      <th>cleaned_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did you hear about the Native American man tha...</td>\n",
       "      <td>He nearly drown in his own tea pee.</td>\n",
       "      <td>[did, you, hear, about, the, native, american,...</td>\n",
       "      <td>[he, nearly, drown, in, his, own, tea, pee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's the best anti diarrheal prescription?</td>\n",
       "      <td>Mycheexarphlexin</td>\n",
       "      <td>[what, is, the, best, anti, diarrheal, prescri...</td>\n",
       "      <td>[mycheexarphlexin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a person who is outside a doo...</td>\n",
       "      <td>Matt</td>\n",
       "      <td>[what, do, you, call, a, person, who, is, outs...</td>\n",
       "      <td>[matt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which Star Trek character is a member of the m...</td>\n",
       "      <td>Jean-Luc Pickacard</td>\n",
       "      <td>[which, star, trek, character, is, a, member, ...</td>\n",
       "      <td>[jeanluc, pickacard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What's the difference between a bullet and a h...</td>\n",
       "      <td>A bullet doesn't miss Harambe</td>\n",
       "      <td>[what, is, the, difference, between, a, bullet...</td>\n",
       "      <td>[a, bullet, does, not, miss, harambe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Did you hear about the Native American man tha...   \n",
       "1       What's the best anti diarrheal prescription?   \n",
       "2  What do you call a person who is outside a doo...   \n",
       "3  Which Star Trek character is a member of the m...   \n",
       "4  What's the difference between a bullet and a h...   \n",
       "\n",
       "                                Answer  \\\n",
       "0  He nearly drown in his own tea pee.   \n",
       "1                     Mycheexarphlexin   \n",
       "2                                 Matt   \n",
       "3                   Jean-Luc Pickacard   \n",
       "4        A bullet doesn't miss Harambe   \n",
       "\n",
       "                                           cleaned_Q  \\\n",
       "0  [did, you, hear, about, the, native, american,...   \n",
       "1  [what, is, the, best, anti, diarrheal, prescri...   \n",
       "2  [what, do, you, call, a, person, who, is, outs...   \n",
       "3  [which, star, trek, character, is, a, member, ...   \n",
       "4  [what, is, the, difference, between, a, bullet...   \n",
       "\n",
       "                                     cleaned_A  \n",
       "0  [he, nearly, drown, in, his, own, tea, pee]  \n",
       "1                           [mycheexarphlexin]  \n",
       "2                                       [matt]  \n",
       "3                         [jeanluc, pickacard]  \n",
       "4        [a, bullet, does, not, miss, harambe]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "tokaniser=RegexpTokenizer(r\"\\w+\")\n",
    "df['cleaned_Q'] = [tokaniser.tokenize(clean_text(sentence)) for sentence in df['Question']]\n",
    "df['cleaned_A'] = [tokaniser.tokenize(clean_text(sentence)) for sentence in df['Answer']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbeae5db-c7b7-432f-b0c3-13ae56d92ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['cleaned_A'].str.len() < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a56362-b91e-48c8-bf7c-6b3268b76cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =df[['cleaned_Q']].apply(lambda words:[\" \".join(word) for word in words]).to_numpy()\n",
    "corpus2 =df[['cleaned_A']].apply(lambda words:[\" \".join(word) for word in words]).to_numpy()\n",
    "corpus = list(corpus.flatten())\n",
    "corpus2 = list(corpus2.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa2ce8-fc80-4cde-bdbf-179eae9efc95",
   "metadata": {},
   "source": [
    "## Creating n-gram tokens and giving them padding according to the max length of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744df04c-0d44-4284-a7c6-9c1bc0344489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to a token sequence \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        input_sequences.append(token_list)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a369be4-9a7f-400b-981a-b84894cfa6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[8, 4, 26, 17, 2, 610, 215, 43, 32, 1759, 2365, 4114, 14, 430],\n",
       "  [1, 6, 2, 63, 4712, 9275, 4713],\n",
       "  [1, 5, 4, 11, 3, 131, 29, 6, 683, 3, 443, 10, 60, 76, 280, 4714, 143],\n",
       "  [111, 331, 2085, 1124, 6, 3, 1011, 14, 2, 1079, 1507],\n",
       "  [1, 6, 2, 21, 20, 3, 2216, 10, 3, 611]],\n",
       " 17667)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sequence,total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequence[:5],total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f28854-cbb8-4edd-b1ed-985216cd0460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36716, 36716)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp_sequence),len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdbde31f-a388-4239-bc1c-a5879a99ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    return predictors, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763b5e1f-34fe-4ec4-be98-a8eda9e9cd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[   0,    0,    0, ..., 2365, 4114,   14],\n",
       "         [   0,    0,    0, ...,   63, 4712, 9275],\n",
       "         [   0,    0,    0, ...,   76,  280, 4714],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,   22, 1919,   12],\n",
       "         [   0,    0,    0, ...,   12,    3,  348],\n",
       "         [   0,    0,    0, ...,    3,  822, 8635]]),\n",
       "  array([ 430, 4713,  143, ...,  697, 3573,  499])),\n",
       " 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, input_len=generate_padded_sequences(inp_sequence)\n",
    "X[:10], input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bba5099-82f2-495f-9f5f-d055f25368ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[18, 7561, 1568, 11, 30, 446, 441, 965],\n",
       "  [19932],\n",
       "  [3859],\n",
       "  [12593, 19933],\n",
       "  [2, 1969, 20, 13, 747, 1852]],\n",
       " 28542)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sequence,total_w = get_sequence_of_tokens(corpus2)\n",
    "out_sequence[:5],total_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2a6b72-147a-4b36-affd-27444357bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d40dbf6-9d1e-459f-81d7-35c976aea6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[    0,     0,     0, ...,    30,   446,   441],\n",
       "         [    0,     0,     0, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    0,     0,     0, ...,    22,   115,    50],\n",
       "         [    0,     0,     0, ...,     0,     2, 28540],\n",
       "         [    0,     0,     0, ...,     0,    29,   489]]),\n",
       "  array([  965, 19932,  3859, ...,  2217, 28541, 13266])),\n",
       " 19)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, out_lens=generate_padded_sequences(out_sequence)\n",
    "y[:10], out_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86484fc0-4765-49d8-8b04-2df576605993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences([str(i) for i in df['cleaned_A']])\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = tf.keras.preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=out_lens , padding='pre' )\n",
    "decoder_output_data = np.array( padded_answers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc967bb7-f34d-4d8f-a268-c2e9e2dc0af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36716, 19), (36716, 22), (36716, 18))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data.shape,X[0].shape,y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335eb34d-1cf8-467e-b67f-1a68cb963f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopper=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903aa068-154e-4c2a-8bb9-1e13a24b3162",
   "metadata": {},
   "source": [
    "## Makeing a simple LSTM RNN Model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64fe9c6c-60ab-4d64-a44f-25e8ce36cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(total_w, 10,input_length=input_len-1))\n",
    "model.add(tf.keras.layers.LSTM(50,return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(50))\n",
    "model.add(tf.keras.layers.Dense(50))\n",
    "model.add(tf.keras.layers.Dense(out_lens-1,activation='relu'))\n",
    "opt_adam = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7705fe8-1b50-42b7-b0e4-a3a0ae95637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt_adam,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d2cc14c-f9f6-4a88-bb55-1efb0c8fbefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 22, 10)            285420    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 22, 50)            12200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 18)                918       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321,288\n",
      "Trainable params: 321,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325ed7e5-857a-49f5-9f9d-fec39b8dd33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "918/918 - 15s - loss: 20862.7168 - accuracy: 0.3814 - val_loss: 16817.6660 - val_accuracy: 0.3637 - 15s/epoch - 16ms/step\n",
      "Epoch 2/30\n",
      "918/918 - 11s - loss: 14250.7266 - accuracy: 0.3825 - val_loss: 15139.0488 - val_accuracy: 0.3637 - 11s/epoch - 12ms/step\n",
      "Epoch 3/30\n",
      "918/918 - 11s - loss: 13786.5684 - accuracy: 0.3821 - val_loss: 14469.5439 - val_accuracy: 0.3660 - 11s/epoch - 12ms/step\n",
      "Epoch 4/30\n",
      "918/918 - 11s - loss: 13123.4336 - accuracy: 0.3829 - val_loss: 14558.5537 - val_accuracy: 0.3568 - 11s/epoch - 12ms/step\n",
      "Epoch 5/30\n",
      "918/918 - 11s - loss: 12865.0518 - accuracy: 0.3884 - val_loss: 14564.6572 - val_accuracy: 0.3637 - 11s/epoch - 12ms/step\n",
      "Epoch 6/30\n",
      "918/918 - 11s - loss: 12720.3496 - accuracy: 0.3906 - val_loss: 14772.6445 - val_accuracy: 0.3632 - 11s/epoch - 12ms/step\n",
      "Epoch 7/30\n",
      "918/918 - 11s - loss: 12559.2959 - accuracy: 0.3876 - val_loss: 14712.1406 - val_accuracy: 0.3637 - 11s/epoch - 12ms/step\n",
      "Epoch 8/30\n",
      "918/918 - 12s - loss: 12395.6709 - accuracy: 0.3891 - val_loss: 15462.7275 - val_accuracy: 0.3429 - 12s/epoch - 13ms/step\n",
      "Epoch 9/30\n",
      "918/918 - 11s - loss: 11994.6455 - accuracy: 0.3938 - val_loss: 16934.0488 - val_accuracy: 0.3408 - 11s/epoch - 12ms/step\n",
      "Epoch 10/30\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "918/918 - 11s - loss: 11801.1221 - accuracy: 0.3953 - val_loss: 16568.5879 - val_accuracy: 0.3606 - 11s/epoch - 12ms/step\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b3b7296d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[0],y[0],epochs=30,validation_split=0.2,verbose=2,callbacks=[earlyStopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d408d1bb-31f6-4736-82cc-6399885f29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, model, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list],maxlen=max_sequence_len-1, padding='pre')\n",
    "    predict_x=model.predict(token_list)[0]\n",
    "    output_word = \"\"\n",
    "    for i in predict_x:\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == int(i):\n",
    "                output_word +=\" \"+ word\n",
    "                break\n",
    "    seed_text += \" -> \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d67ea372-6b4f-4b1b-8511-7880691e8d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What Pokemon Can You Find At Auschwitz ->  The A'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand=np.random.randint(0,high=df['cleaned_Q'].shape[0])\n",
    "generate_text(clean_text(df['Question'][rand]),model,input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258c91d-beca-4bc9-a6f9-0f0c5a0c7a22",
   "metadata": {},
   "source": [
    "## The loss was very high and low accuracy for the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36155d6-87dc-477a-94ff-dba9eecc8aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
